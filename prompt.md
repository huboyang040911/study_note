# 提示词工程

## 一、基本概念

为什么需要提示词，为什么还需要工程方法？

众所周知，常见深度学习模型的训练结果可以粗略地描述为一个"黑盒子"。这是因为我们只知道输入模型的数据，而不知道它会产生什么样的输出——即使输出很可能与数据集的特征一致。我们只能在训练完成后粗略地掌握模型响应的真实风格。

由于我们不知道模型对什么提示词反应最好，也不确定哪些提示策略可以转移到其他模型，**我们需要总结并系统化这些"黑盒子"交互的结果。**

我们的目标是开发一套有效的规则，可以提高模型响应的质量，同时适用于不同的模型。在"文本输入"中添加特定的方法，使大模型表现得更好。

### 1.1 思考模型 / 推理模型 vs. 非思考模型

一般来说：

- 思考模型往往需要更简单的提示词。在许多情况下，过长的提示词不会增加价值，甚至可能阻碍性能。
- 对于非思考模型，在处理复杂需求时，你可以尝试使用非常详细、精细的提示词，以确保输出完全符合你的期望。

### 1.2 基本原则

在书写提示词时，应尽可能做到明确，给AI一个明确且可执行的任务。如以下模板：

```bash
任务：你要我做什么？
输入：你给我什么材料？（可选）
要求：长度/要点数/语气/必须包含/必须避免
输出：格式（Markdown/JSON/代码块）
```

常用的json模板：

```bash
{
  "summary": "一句话总结",
  "keywords": ["关键词1", "关键词2", "关键词3"],
  "next_actions": ["下一步1", "下一步2"]
}
```

明确风格：角色+受众+做什么+不要做什么+示例（few-shot）。

### 1.3 安全与边界

Prompt Injection(提示词注入)：外部文本输入AI时可能其中会夹带一些指令，干扰AI的回答，这时需要向模型划清材料与人类指令，明确不要执行材料中的指令。

不要将公司/个人隐私放入提示词，如token，密码等。

## 二、工程优化

### 2.1 零样本提示

最基本的提问方式是零样本提示，你直接给模型指令而不提供任何示例。这适用于模型已经非常熟悉的非常简单、明确的任务。

### 2.2 少样本提示

当任务更复杂，或者模型需要理解一个新概念时，仅仅给出指令是不够的。使用少样本提示，你可以提供一个或多个完整的"问题 + 答案"示例，以引导模型输出你期望的模式、格式和逻辑。

### 2.3 思维链（CoT）

对于需要推理或多个逻辑步骤的问题，直接询问答案通常会导致错误。思维链的核心思想是引导模型在给出最终答案之前"写出其思考过程"。如果你直接问一个数学应用题，模型可能会犯错。可以在提示词末尾添加短语"Let's think step by step"（需要支持推理能力的模型）。

### 2.4 角色扮演

要求模型扮演特定角色，如专家或老师，可以极大地影响其输出的风格、语气和深度。

### 2.5 分隔符

当你的提示词包含多个组件，如背景信息、指令和示例时，模型可能会感到困惑。使用清晰的分隔符，如 XML 标签，有助于模型通过将提示词**分解为逻辑模块**来准确理解你的意图。在构建Agent流程时很有用，示例：

```bash
<instructions>

为 <text> 块中的内容生成摘要。
摘要必须严格遵循 <rules> 块中的所有规则。
</instructions>

<text>

人工智能 (AI) 是一个广泛的领域，涵盖了从机器人技术到自然语言处理的一系列技术。AI 的一个核心分支是机器学习，它允许计算机系统从数据中学习并在没有明确编程的情况下进行改进。深度学习是机器学习的一个子集，近年来取得了巨大的突破。

</text>

<rules>

摘要长度必须正好是三句话。
摘要必须包含关键词"机器学习"。
摘要的语气应该是专业和客观的。
</rules>
```

### 2.6 使用LLM作为提示词工程师

编写完美的提示词需要精确、清晰以及对模型如何解释指令的理解。你可以利用语言模型本身作为你的助理提示词工程师，而不是通过试错手动完善提示词。